Docker vllm部署

# 不含LoRA启动

## 生产环境

```shell
docker run -itd --runtime nvidia \
    --gpus='"device=0"' \
    -v /home/ubuntu/Qwen2.5-32B-018-awq-0718/:/Qwen2.5-32B-018-awq-0718/ \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.9.1 \
    --model /Qwen2.5-32B-018-awq-0718 \
    --served-model-name qwen2.5-32B \
    --tensor-parallel-size 1 \
    --max_model_len 8192 \
    --api-key NEsepnJ5eXxOm44k \
    --gpu_memory_utilization 0.95 \
    --enforce-eager
```



## 开发环境

```shell
docker run -itd --runtime nvidia \
    --gpus='"device=1"' \
    -v /mnt/6d5d6f24-68f7-4fc0-88f7-ab4ad5c540ac/data/yw/PRODUCTION/Qwen2.5-7B-018-0801/:/Qwen2.5-7B-018-0801/ \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.3 \
    --model /Qwen2.5-7B-018-0801 \
    --served-model-name qwen2.5-7B \
    --tensor-parallel-size 1 \
    --max_model_len 8192 \
    --api-key NEsepnJ5eXxOm44k \
    --gpu_memory_utilization 0.95 \
    --enforce-eager
```



测试:

```shell
docker run -v /mnt/6d5d6f24-68f7-4fc0-88f7-ab4ad5c540ac/data/yw/models/MN-Violet-Lotus-12B.Q8_0.gguf:/models/MN-Violet-Lotus-12B.Q8_0.gguf ghcr.io/ggml-org/llama.cpp:full --run -m /models/MN-Violet-Lotus-12B.Q8_0.gguf

docker run -itd --runtime nvidia \
    --gpus='"device=3"' \
    -v /mnt/6d5d6f24-68f7-4fc0-88f7-ab4ad5c540ac/data/yw/models/Peach-2.0-9B-8k-Roleplay/:/Peach-2.0-9B-8k-Roleplay/ \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.3 \
    --model /Peach-2.0-9B-8k-Roleplay \
    --served-model-name llm \
    --tensor-parallel-size 1 \
    --max_model_len 8192 \
    --api-key NEsepnJ5eXxOm44k \
    --gpu_memory_utilization 0.95 \
    --enforce-eager
```



# 含LoRA启动





















***

***

***

以下为研究

# 参数

显存不足的时候加上https://zhuanlan.zhihu.com/p/3722264996#:\~:text=vLLM%E7%89%88%E6%9C%AC%E9%80%89%E6%8B%A9%E3%80%82-,3%E3%80%81enforce%2Deager,-enforce\_eager%E6%98%AF%E4%B8%80%E4%B8%AA

## 环境安装

vllm

**务必将所有的加速包都安装上，不然无法高效用GPU**

```shell
conda activate vllm

# nvcc -V查看cu版本
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu126
pip install accelerate
pip install transformers
pip install optimum
pip install auto-gptq 或
pip install autoawq
pip install flashinfer-python

版本如下：
vllm==0.9.1
transformers==4.53.0
torch==2.7.0+cu126
torchaudio==2.7.0+cu126
torchvision==0.22.0+cu126
optimum==1.26.1
flashinfer-python==0.2.7.post1
autoawq==0.2.9
accelerate==1.8.1
```

## 模型传输：

```shell
# 数据服务器到跳板机
scp -i chev-jump-key.pem -r xxx/ ec2-user@3.144.48.202:/mnt/efs/algo/datasets/xxx/
cp -r /mnt/efs/algo/datasets/xxx/ /mnt/s3/public/llm/xxx/

# 跳板机到大模型机器
scp -i  /data/remote/chev-eks-node-key.pem -r ./xxx ec2-user@10.2.166.209:/home/ec2-user/xxx
```

### vllm部署：

```shell
# 模型位置：工作站PRODUCTION目录,/mnt/6d5d6f24-68f7-4fc0-88f7-ab4ad5c540ac/data/yw/PRODUCTION

# 部署全参模型：
# 1、使用conda环境：
vllm serve Qwen2.5-32B-015-0623 --tensor-parallel-size 4 --gpu_memory_utilization 0.95 --max_model_len 8192
# 2、使用docker vllm镜像：


# GPTQ8或AWQ量化模型部署
# 1、使用conda环境：
# GPTQ8模型
nohup vllm serve xxx \
    --served-model-name qwen2.5-32B
    --tensor-parallel-size 2 \
    --max_model_len 8192 \
    --disable-custom-all-reduce true \
    --api-key NEsepnJ5eXxOm44k \
    --host 0.0.0.0 \
    --port 8000 \
    > vllm_0630.log 2>&1 &

# awq模型
nohup vllm serve Qwen2.5-32B-014-awq-0630 \
    --served-model-name qwen2.5-32B \
    --tensor-parallel-size 1 \
    --max_model_len 4096 \
    --api-key NEsepnJ5eXxOm44k \
    --gpu_memory_utilization 0.95 \
    --host 0.0.0.0 \
    --port 8000 \
    --enforce-eager \
    > vllm_0701.log 2>&1 &

# 2、使用docker vllm镜像：
# docker部署：
docker run -itd --runtime nvidia \
    --gpus='"device=1"' \
    -v /mnt/6d5d6f24-68f7-4fc0-88f7-ab4ad5c540ac/data/yw/PRODUCTION/Qwen2.5-32B-018-awq-0718/:/Qwen2.5-32B-018-awq-0718/ \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:v0.7.3 \
    --model /Qwen2.5-32B-018-awq-0718 \
    --served-model-name qwen2.5-32B \
    --tensor-parallel-size 1 \
    --max_model_len 8192 \
    --api-key NEsepnJ5eXxOm44k \
    --gpu_memory_utilization 0.95 \
    --enforce-eager
```

### sglang-conda部署：

```shell
# 全精度模型部署
nohup python3 -m sglang.launch_server \
    --model qwen2.5-32B \
    --trust-remote-code \
    --tp 4 \
    --schedule-conservativeness 1.3 \
    --api-key NEsepnJ5eXxOm44k \
    --host 0.0.0.0 \
    --port 8000 \
    > sglang_log_0629.log 2>&1 &

# 量化模型部署
python3 -m sglang.launch_server \
    --model Qwen2.5-32B-aws-gptq8-0623 \
    --trust-remote-code \
    --tp 4 \
    --schedule-conservativeness 1.3 \
    --port 8000 \
    --host 0.0.0.0
```

