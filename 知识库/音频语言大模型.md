# 1、端到端语音大模型



# 2、支持音频模态的大模型





[nari-labs/Dia-1.6B](https://huggingface.co/nari-labs/Dia-1.6B)：TTS产品，特点：可以在音频上增加条件来控制语音情感和语调，还能生成如笑声、咳嗽、清嗓子等
[Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)：音频理解模型（输入音频or文本，输出文本），特点：比ASR能力更高阶（支持输入说话人语音、自然音、音乐、歌声）
[mini-omni](https://github.com/gpt-omni/mini-omni)：音频端到端模型（语音输入和音频输出），特点：仅支持英文输出，不支持实时打断
[MinMo](https://funaudiollm.github.io/minmo/)：实时音频端到端模型，特点：支持多语种和实时打断，暂未开源权重



[Qwen/Qwen2.5-Omni-7B](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)：

* [简介](https://help.aliyun.com/zh/model-studio/qwen-omni)：相比于 [Qwen-VL](https://help.aliyun.com/zh/model-studio/vision) 与 [Qwen-Audio](https://help.aliyun.com/zh/model-studio/audio-language-model) 模型，Qwen-Omni 模型可以：

  * 理解视频文件中的视觉与音频信息；

  * 理解多种模态的数据；

  * 输出音频。

* 多模态大模型

* 特点：

  * 支持语音图片视频文本输入（音频和视频是离线输入，暂不支持实时通话，或者未开源），流式文本和语音输出

  * **非实时通话，在音视频通话时需要将音视频文件离线上传**

* 输出模态包括音频时**不支持设定 System Message（**&#x5373;使您在 System Message 中说明：“你是XXX...”等角色信息，Qwen-Omni 的自我认知依然会是通义千问。**）**
