学习教程：

https://github.com/datawhalechina/easy-rl

主要目标：

使用强化学习来做角色扮演的最佳方案到底是什么样？

角色扮演如何设计奖励函数？



https://www.kimi.com/chat/d3jtmed7vds0ec99kk6g

https://mp.weixin.qq.com/s?__biz=MzUyMDc5OTU5NA==&mid=2247711452&idx=3&sn=592f855349e20e1eef197dc1c86bdbf1&poc_token=HJvH52ijdf5TFNj_UDxZfi9KRGH4GQpUeAQ3hqgY

https://blog.csdn.net/m0_65555479/article/details/152324136

https://zhuanlan.zhihu.com/p/25985130568

https://zhuanlan.zhihu.com/p/28440962040





# 知识点

## 为什么奖励稀疏是一个大问题？

https://chat.deepseek.com/a/chat/s/04d0b45e-a2bf-4615-995e-ea0cebc59743



## GRPO是什么？

GRPO 是一种在线学习算法，这意味着它会在训练过程中使用训练模型自身生成的数据进行迭代改进。GRPO 目标背后的直觉是最大化生成的补全的优势，同时确保模型接近参考策略。为了理解 GRPO 的工作原理，可以将其分解为四个主要步骤： **生成补全** 、 **计算优势** 、 **估计 KL 散度**以及**计算损失** 。

![img](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png)

重要链接：

https://huggingface.co/docs/trl/v0.16.1/en/grpo_trainer#looking-deeper-into-the-grpo-method



## GRPO的损失函数是什么？

https://www.kimi.com/chat/d3jsb4fpfah3is2t4b3g

<img src="./image1.png" alt="image-20251009231849136" style="zoom: 33%;" />



## GRPO中的策略参考和时间t

策略函数：需要优化和训练的大模型，大模型会根据当前系统提示词和上下文输出内容。

参考函数：通常是指参考模型的输出函数，参考模型一般是初始的监督微调模型

在大语言模型领域的RLHF-GRPO中，策略函数、参考函数和时间t的含义如下：

### 策略函数

- **定义**：策略函数是智能体（大语言模型）根据当前状态（如输入的提示词或上下文）来决定采取何种动作（如生成下一个词或完整的回答）的函数。
- **作用**：在RLHF-GRPO中，策略函数是需要被优化的核心部分，它决定了模型如何根据输入生成最符合人类偏好的输出。

### 参考函数

- **定义**：参考函数通常是指参考模型的输出函数，参考模型一般是初始的监督微调模型。
- **作用**：在训练过程中，参考函数用于提供一个基准，帮助约束策略函数的更新，防止策略函数偏离初始的监督微调结果太远，从而保持模型的稳定性和语言能力。

### 时间t

- **定义**：在强化学习的上下文中，时间t通常表示决策过程中的时间步。
- **作用**：在大语言模型生成文本的过程中，时间t可以对应于生成文本的每个步骤，例如生成下一个词或token的时刻。



https://www.kimi.com/chat/d3jt7dvhvltt3clce3tg



[ShareGPT格式的数据来做DPO](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md#%E5%81%8F%E5%A5%BD%E6%95%B0%E6%8D%AE%E9%9B%86-1)



# 分类

GRPO

GRPO 最早由 DeepSeek 团队在其论文《[**DeepSeekMath**](https://zhuanlan.zhihu.com/p/20646704714): Pushing the Limits of Mathematical Reasoning in Open Language Models》中提出，旨在解决传统 RL 方法在 LLM 训练中的局限性。

[GRPO教程](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/)


DPO

# 训练框架

1、https://github.com/hiyouga/EasyR1

实践：https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/



2、trl框架训练grpo

https://huggingface.co/docs/trl/v0.16.1/en/grpo_trainer

问题：



文章：

相对偏好优化

https://arxiv.org/html/2402.10958v1



RLPR：又一种「相信直觉」的 LLM 强化学习范式

https://zhuanlan.zhihu.com/p/1921591317823218365

