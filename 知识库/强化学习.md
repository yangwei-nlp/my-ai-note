学习教程：

https://github.com/datawhalechina/easy-rl

主要目标：

使用强化学习来做角色扮演的最佳方案到底是什么样？

角色扮演如何设计奖励函数？



# 问题

### 为什么奖励稀疏是一个大问题？

https://chat.deepseek.com/a/chat/s/04d0b45e-a2bf-4615-995e-ea0cebc59743





[ShareGPT格式的数据来做DPO](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/README_zh.md#%E5%81%8F%E5%A5%BD%E6%95%B0%E6%8D%AE%E9%9B%86-1)



# 分类

GRPO

GRPO 最早由 DeepSeek 团队在其论文《[**DeepSeekMath**](https://zhuanlan.zhihu.com/p/20646704714): Pushing the Limits of Mathematical Reasoning in Open Language Models》中提出，旨在解决传统 RL 方法在 LLM 训练中的局限性。

[GRPO教程](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/)


DPO


# 训练框架

https://github.com/hiyouga/EasyR1

实践：https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/





文章：

相对偏好优化

https://arxiv.org/html/2402.10958v1



RLPR：又一种「相信直觉」的 LLM 强化学习范式

https://zhuanlan.zhihu.com/p/1921591317823218365

